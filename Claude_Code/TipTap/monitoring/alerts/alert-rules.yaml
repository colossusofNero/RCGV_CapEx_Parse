# TipTap Production Alert Rules Configuration
# Critical alerts for production monitoring with appropriate thresholds

version: "1.0"
namespace: "tiptap-production"

alert_groups:
  # Payment System Alerts
  payment_alerts:
    name: "Payment System Alerts"
    interval: "30s"
    rules:
      - alert: "HighPaymentFailureRate"
        expr: |
          (sum(rate(payment_failures_total[5m])) / sum(rate(payment_attempts_total[5m]))) * 100 > 5
        for: "2m"
        severity: "critical"
        labels:
          team: "payments"
          service: "tiptap"
        annotations:
          summary: "Payment failure rate is above 5%"
          description: |
            Payment failure rate has been {{ $value | humanizePercentage }} for more than 2 minutes.
            This affects user experience and revenue.
          runbook_url: "https://docs.tiptap.com/runbooks/payment-failures"
        notification_channels: ["slack-payments", "email-oncall", "pagerduty"]

      - alert: "PaymentMethodDown"
        expr: |
          (sum(rate(payment_failures_total[5m])) by (payment_method) /
           sum(rate(payment_attempts_total[5m])) by (payment_method)) * 100 > 25
        for: "1m"
        severity: "critical"
        labels:
          team: "payments"
          service: "tiptap"
        annotations:
          summary: "Payment method {{ $labels.payment_method }} is experiencing high failure rate"
          description: |
            Payment method {{ $labels.payment_method }} has {{ $value | humanizePercentage }} failure rate.
          runbook_url: "https://docs.tiptap.com/runbooks/payment-method-issues"
        notification_channels: ["slack-payments", "pagerduty"]

      - alert: "NFCPaymentLatency"
        expr: |
          histogram_quantile(0.95, sum(rate(nfc_transaction_duration_seconds_bucket[5m])) by (le)) > 3
        for: "3m"
        severity: "warning"
        labels:
          team: "mobile"
          service: "tiptap"
          payment_method: "nfc"
        annotations:
          summary: "NFC payment latency is high"
          description: |
            95th percentile NFC transaction time is {{ $value }}s, above 3s threshold.
        notification_channels: ["slack-mobile"]

  # API Performance Alerts
  api_performance_alerts:
    name: "API Performance Alerts"
    interval: "30s"
    rules:
      - alert: "HighAPILatency"
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, endpoint)) > 2
        for: "2m"
        severity: "warning"
        labels:
          team: "backend"
          service: "tiptap-api"
        annotations:
          summary: "API endpoint {{ $labels.endpoint }} has high latency"
          description: |
            95th percentile response time for {{ $labels.endpoint }} is {{ $value }}s.
          runbook_url: "https://docs.tiptap.com/runbooks/api-performance"
        notification_channels: ["slack-backend"]

      - alert: "APIResponseTimeSpike"
        expr: |
          (
            histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[2m])) by (le)) /
            histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[10m] offset 10m)) by (le))
          ) > 2
        for: "1m"
        severity: "warning"
        labels:
          team: "backend"
          service: "tiptap-api"
        annotations:
          summary: "API response time has spiked significantly"
          description: |
            Current median response time is {{ $value }}x higher than 10 minutes ago.
        notification_channels: ["slack-backend"]

  # System Error Alerts
  system_error_alerts:
    name: "System Error Alerts"
    interval: "30s"
    rules:
      - alert: "HighServerErrorRate"
        expr: |
          (sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))) * 100 > 1
        for: "2m"
        severity: "critical"
        labels:
          team: "backend"
          service: "tiptap"
        annotations:
          summary: "Server error rate is above 1%"
          description: |
            5xx error rate is {{ $value | humanizePercentage }} for the last 5 minutes.
          runbook_url: "https://docs.tiptap.com/runbooks/server-errors"
        notification_channels: ["slack-backend", "pagerduty"]

      - alert: "DatabaseConnectionFailure"
        expr: |
          sum(rate(database_connection_failures_total[5m])) > 0
        for: "30s"
        severity: "critical"
        labels:
          team: "infrastructure"
          service: "database"
        annotations:
          summary: "Database connection failures detected"
          description: |
            Database connection failures: {{ $value }} per second.
        notification_channels: ["slack-infrastructure", "pagerduty"]

      - alert: "FeatureErrorSpike"
        expr: |
          (
            sum(rate(feature_errors_total[2m])) by (feature) /
            sum(rate(feature_errors_total[10m] offset 10m)) by (feature)
          ) > 5
        for: "1m"
        severity: "warning"
        labels:
          team: "product"
          service: "tiptap"
        annotations:
          summary: "Error spike in feature {{ $labels.feature }}"
          description: |
            Feature {{ $labels.feature }} errors are {{ $value }}x higher than normal.
        notification_channels: ["slack-product"]

  # Fraud Detection Alerts
  fraud_detection_alerts:
    name: "Fraud Detection Alerts"
    interval: "1m"
    rules:
      - alert: "FraudScoreSpike"
        expr: |
          avg(fraud_score_current) > avg(fraud_score_baseline[24h]) * 2
        for: "2m"
        severity: "critical"
        labels:
          team: "security"
          service: "fraud-detection"
        annotations:
          summary: "Fraud score spike detected"
          description: |
            Current fraud score ({{ $value }}) is significantly higher than baseline.
          runbook_url: "https://docs.tiptap.com/runbooks/fraud-detection"
        notification_channels: ["slack-security", "email-security-team"]

      - alert: "SuspiciousTransactionPattern"
        expr: |
          sum(rate(suspicious_transactions_total[5m])) > 10
        for: "1m"
        severity: "warning"
        labels:
          team: "security"
          service: "fraud-detection"
        annotations:
          summary: "High rate of suspicious transactions"
          description: |
            {{ $value }} suspicious transactions per second detected.
        notification_channels: ["slack-security"]

      - alert: "MultipleFailedAuthAttempts"
        expr: |
          sum(rate(auth_failures_total[5m])) by (user_id) > 5
        for: "1m"
        severity: "warning"
        labels:
          team: "security"
          service: "authentication"
        annotations:
          summary: "Multiple failed authentication attempts"
          description: |
            User {{ $labels.user_id }} has {{ $value }} failed auth attempts per second.
        notification_channels: ["slack-security"]

  # Business Metrics Alerts
  business_alerts:
    name: "Business Metrics Alerts"
    interval: "1m"
    rules:
      - alert: "LowDailyActiveUsers"
        expr: |
          count(count by (user_id) (increase(user_activity_total[24h]))) < 1000
        for: "1h"
        severity: "warning"
        labels:
          team: "product"
          service: "tiptap"
        annotations:
          summary: "Daily active users below threshold"
          description: |
            Only {{ $value }} daily active users, below 1000 threshold.
        notification_channels: ["slack-product"]

      - alert: "TipVolumeDropSignificant"
        expr: |
          sum(increase(tip_amount_total[1h])) < sum(increase(tip_amount_total[1h] offset 24h)) * 0.5
        for: "2h"
        severity: "warning"
        labels:
          team: "product"
          service: "tiptap"
        annotations:
          summary: "Significant drop in tip volume"
          description: |
            Current hourly tip volume is less than 50% of same hour yesterday.
        notification_channels: ["slack-product", "email-management"]

  # Infrastructure Alerts
  infrastructure_alerts:
    name: "Infrastructure Alerts"
    interval: "30s"
    rules:
      - alert: "HighCPUUsage"
        expr: |
          avg(cpu_usage_percent) by (instance) > 80
        for: "5m"
        severity: "warning"
        labels:
          team: "infrastructure"
          service: "tiptap"
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: |
            CPU usage is {{ $value }}% on instance {{ $labels.instance }}.
        notification_channels: ["slack-infrastructure"]

      - alert: "HighMemoryUsage"
        expr: |
          (memory_used_bytes / memory_total_bytes) * 100 > 85
        for: "5m"
        severity: "warning"
        labels:
          team: "infrastructure"
          service: "tiptap"
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: |
            Memory usage is {{ $value }}% on instance {{ $labels.instance }}.
        notification_channels: ["slack-infrastructure"]

      - alert: "LowBalanceAlert"
        expr: |
          account_balance_usd < 1000
        for: "1m"
        severity: "critical"
        labels:
          team: "finance"
          service: "payment-processor"
        annotations:
          summary: "Account balance is critically low"
          description: |
            Account balance is ${{ $value }}, below $1000 threshold.
          runbook_url: "https://docs.tiptap.com/runbooks/low-balance"
        notification_channels: ["slack-finance", "email-finance-team", "sms-oncall"]

      - alert: "ServiceDown"
        expr: |
          up == 0
        for: "1m"
        severity: "critical"
        labels:
          team: "infrastructure"
        annotations:
          summary: "Service {{ $labels.instance }} is down"
          description: |
            Service {{ $labels.job }} on {{ $labels.instance }} is not responding.
        notification_channels: ["pagerduty", "slack-infrastructure"]

# Global alert configuration
global:
  resolve_timeout: "5m"
  smtp_smarthost: "smtp.tiptap.com:587"
  smtp_from: "alerts@tiptap.com"

# Notification channel configurations
notification_channels:
  slack-payments:
    type: "slack"
    webhook_url: "{{ .SlackWebhookPayments }}"
    channel: "#payments-alerts"
    title: "TipTap Payment Alert"

  slack-backend:
    type: "slack"
    webhook_url: "{{ .SlackWebhookBackend }}"
    channel: "#backend-alerts"
    title: "TipTap Backend Alert"

  slack-mobile:
    type: "slack"
    webhook_url: "{{ .SlackWebhookMobile }}"
    channel: "#mobile-alerts"
    title: "TipTap Mobile Alert"

  slack-security:
    type: "slack"
    webhook_url: "{{ .SlackWebhookSecurity }}"
    channel: "#security-alerts"
    title: "TipTap Security Alert"

  slack-product:
    type: "slack"
    webhook_url: "{{ .SlackWebhookProduct }}"
    channel: "#product-alerts"
    title: "TipTap Product Alert"

  slack-infrastructure:
    type: "slack"
    webhook_url: "{{ .SlackWebhookInfra }}"
    channel: "#infrastructure-alerts"
    title: "TipTap Infrastructure Alert"

  slack-finance:
    type: "slack"
    webhook_url: "{{ .SlackWebhookFinance }}"
    channel: "#finance-alerts"
    title: "TipTap Finance Alert"

  pagerduty:
    type: "pagerduty"
    service_key: "{{ .PagerDutyServiceKey }}"

  email-oncall:
    type: "email"
    to: ["oncall@tiptap.com"]
    subject: "TipTap Production Alert"

  email-security-team:
    type: "email"
    to: ["security@tiptap.com"]
    subject: "TipTap Security Alert"

  email-finance-team:
    type: "email"
    to: ["finance@tiptap.com"]
    subject: "TipTap Finance Alert"

  email-management:
    type: "email"
    to: ["management@tiptap.com"]
    subject: "TipTap Business Alert"

  sms-oncall:
    type: "sms"
    to: ["{{ .OnCallPhoneNumber }}"]

# Alert routing rules
route:
  group_by: ["alertname", "service"]
  group_wait: "30s"
  group_interval: "2m"
  repeat_interval: "4h"
  receiver: "default"
  routes:
    - match:
        severity: "critical"
      receiver: "critical-alerts"
      group_wait: "10s"
      repeat_interval: "1h"
    - match:
        team: "security"
      receiver: "security-alerts"
    - match:
        team: "finance"
      receiver: "finance-alerts"

receivers:
  - name: "default"
    slack_configs:
      - channel: "#general-alerts"
  - name: "critical-alerts"
    pagerduty_configs:
      - service_key: "{{ .PagerDutyServiceKey }}"
    email_configs:
      - to: ["oncall@tiptap.com"]
  - name: "security-alerts"
    slack_configs:
      - channel: "#security-alerts"
    email_configs:
      - to: ["security@tiptap.com"]
  - name: "finance-alerts"
    slack_configs:
      - channel: "#finance-alerts"
    email_configs:
      - to: ["finance@tiptap.com"]
    sms_configs:
      - to: ["{{ .OnCallPhoneNumber }}"]